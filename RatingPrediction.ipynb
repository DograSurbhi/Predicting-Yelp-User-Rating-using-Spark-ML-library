{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='6000m'\n",
    "launcher.packages=[\"com.github.master:spark-stemming_2.10:0.2.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm:8088/proxy/application_1574735054766_0001\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = yarn, app id = application_1574735054766_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "reviewsDF: org.apache.spark.sql.DataFrame = [text: string, stars: double]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Data Exploration\n",
    "val reviewsDF=spark.read.json(\"/hadoop-user/data/review.json\").select(\"text\", \"stars\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|stars|\n",
      "+--------------------+-----+\n",
      "|Total bill for th...|  1.0|\n",
      "|I *adore* Travis ...|  5.0|\n",
      "|I have to say tha...|  5.0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res0: reviewsDF.type = [text: string, stars: double]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewsDF.show(3)\n",
    "reviewsDF.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|stars|  count|\n",
      "+-----+-------+\n",
      "|  1.0|1002159|\n",
      "|  4.0|1468985|\n",
      "|  3.0| 739280|\n",
      "|  2.0| 542394|\n",
      "|  5.0|2933082|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviewsDF.createOrReplaceTempView(\"reviews\")\n",
    "//the number of reviews for each star value\n",
    "spark.sql(\"select stars,count(text) as count from reviews group by stars\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|                text|stars|rating|\n",
      "+--------------------+-----+------+\n",
      "|Total bill for th...|  1.0|   0.0|\n",
      "|I *adore* Travis ...|  5.0|   1.0|\n",
      "|I have to say tha...|  5.0|   1.0|\n",
      "+--------------------+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Bucketizer\n",
       "splits: Array[Double] = Array(0.0, 4.0, 5.0)\n",
       "bucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_852744a0bdc1\n",
       "bucketedData: org.apache.spark.sql.DataFrame = [text: string, stars: double ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Feature Engineering \n",
    "//Bucketing to create a new column “rating” with values 0 (if the star rating is 1,2, or 3) and 1 (if the star rating is 4 or 5)\n",
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val splits = Array(0,4,5.toDouble)\n",
    "\n",
    "\n",
    "val bucketizer = new Bucketizer()\n",
    "  .setInputCol(\"stars\")\n",
    "  .setOutputCol(\"rating\")\n",
    "  .setSplits(splits)\n",
    "\n",
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(reviewsDF)\n",
    "bucketedData.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|   0.0|2283833|\n",
      "|   1.0|4402067|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketedData.createOrReplaceTempView(\"bucketed_reviews\")\n",
    "// the count of reviews for each rating value\n",
    "spark.sql(\"select rating,count(text) as count from bucketed_reviews group by rating\").show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|                text|stars|rating|\n",
      "+--------------------+-----+------+\n",
      "|This place has go...|  1.0|   0.0|\n",
      "|So good! They did...|  4.0|   1.0|\n",
      "|Met an old close ...|  4.0|   1.0|\n",
      "+--------------------+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fractionKeyMap: scala.collection.immutable.Map[Int,Double] = Map(0 -> 0.1, 1 -> 0.05)\n",
       "StratifiedSample: org.apache.spark.sql.DataFrame = [text: string, stars: double ... 1 more field]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Stratified Sampling and get a sample of only 10% of reviews in each rating category after down-sampling\n",
    "\n",
    "val fractionKeyMap = Map(0 -> 0.1, 1-> 0.1*0.5)\n",
    "\n",
    "val StratifiedSample= bucketedData.stat.sampleBy(\"rating\", fractionKeyMap, 7L)\n",
    "\n",
    "\n",
    "StratifiedSample.show(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.mllib.feature.Stemmer\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_9b07384e9a4f\n",
       "import org.apache.spark.sql.functions.udf\n",
       "removePunc: (words: Seq[String])Seq[String]\n",
       "puncRemover: org.apache.spark.ml.feature.SQLTransformer = sql_15288d7b21dc\n",
       "stopWordRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_450a33bb42e6\n",
       "stemmer: org.apache.spark.mllib.feature.Stemmer = stemmer_069028e38708\n",
       "vectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_229899540b4d\n",
       "tfidf: org.apache.spark.ml.feature.IDF = idf_c078406b1a0d\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Extracting TFIDF vectors from the review Text\n",
    "\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.mllib.feature.Stemmer\n",
    "\n",
    "//tokenising\n",
    "val tokenizer = new RegexTokenizer().setMinTokenLength(3).setToLowercase(true).setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "//Defining a udf to remove punctuations from a sequence of words\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "def removePunc(words:Seq[String]):Seq[String]={\n",
    " return words.map(_.replaceAll(\"\\\\p{Punct}\",\" \"))\n",
    "}\n",
    "\n",
    "//val removePuncUDF=udf(removePunc(_:Seq[String]))\n",
    "spark.udf.register(\"removePuncUDF\",removePunc(_:Seq[String]) )\n",
    "\n",
    "//use the removePuncUDF to remove all punctuations from words\n",
    "val puncRemover = new SQLTransformer().setStatement(\"SELECT removePuncUDF(words) as text, rating from __THIS__ \")\n",
    "\n",
    "//removing stop words using spark's StopWordRemover\n",
    "val stopWordRemover=new StopWordsRemover().setInputCol(\"text\").setOutputCol(\"filtered_text\")\n",
    "\n",
    "////Stemming using a third party package\n",
    "val stemmer = new Stemmer().setInputCol(\"filtered_text\").setOutputCol(\"stemmed_text\")\n",
    "\n",
    "\n",
    "val vectorizer = new CountVectorizer().setInputCol(\"stemmed_text\").setOutputCol(\"BOW\").setMinDF(100)\n",
    "\n",
    "//final tfidf vector\n",
    "val tfidf = new IDF().setInputCol(\"BOW\").setOutputCol(\"text_TFIDF\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------------------+\n",
      "|rating|prediction|         probability|        stemmed_text|\n",
      "+------+----------+--------------------+--------------------+\n",
      "|   1.0|       1.0|[0.04664469386640...|[ original , take...|\n",
      "|   0.0|       0.0|[0.91037025362698...|[       , hairdre...|\n",
      "|   1.0|       0.0|[0.67643752066498...|[ 10 99, champagn...|\n",
      "|   0.0|       0.0|[0.99594554906796...|[ 148, dinner, gu...|\n",
      "|   0.0|       0.0|[0.99680105550227...|[small, drip , ta...|\n",
      "+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for LR on test data = 0.948347173215654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_8ee311034225\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_8ee311034225-elasticNetParam: 0.0,\n",
       "\tlogreg_8ee311034225-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_8ee311034225-elasticNetParam: 0.5,\n",
       "\tlogreg_8ee311034225-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_8ee311034225-elasticNetParam: 1.0,\n",
       "\tlogreg_8ee311034225-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_8ee311034225-elasticNetParam: 0.0,\n",
       "\tlogreg_8ee311034225-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_8ee311034225-elasticNetParam: 0.5,\n",
       "\tlogreg_8ee311034225-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_8..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Building Machine Learning pipelines\n",
    "//Logistic Regression\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "val lr = new LogisticRegression().setLabelCol(\"rating\").setFeaturesCol(\"text_TFIDF\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             \n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "//creating a pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf,cv))\n",
    "\n",
    "//dividing data into training and test data\n",
    "val Array(training,testing)=StratifiedSample.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"rating\", \"prediction\", \"probability\", \"stemmed_text\").show(5)\n",
    "\n",
    "//Evaluating our model\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------------------+\n",
      "|rating|prediction|         probability|        stemmed_text|\n",
      "+------+----------+--------------------+--------------------+\n",
      "|   1.0|       1.0|[0.48017184576031...|[ original , take...|\n",
      "|   0.0|       0.0|[0.50467679605114...|[       , hairdre...|\n",
      "|   1.0|       0.0|[0.56551560174407...|[ 10 99, champagn...|\n",
      "|   0.0|       0.0|[0.58987161890653...|[ 148, dinner, gu...|\n",
      "|   0.0|       0.0|[0.55671834440989...|[small, drip , ta...|\n",
      "+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for RF on test data = 0.8059054542735092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_83716b7d4aac\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_83716b7d4aac-maxDepth: 2,\n",
       "\trfc_83716b7d4aac-numTrees: 5\n",
       "}, {\n",
       "\trfc_83716b7d4aac-maxDepth: 5,\n",
       "\trfc_83716b7d4aac-numTrees: 5\n",
       "}, {\n",
       "\trfc_83716b7d4aac-maxDepth: 2,\n",
       "\trfc_83716b7d4aac-numTrees: 20\n",
       "}, {\n",
       "\trfc_83716b7d4aac-maxDepth: 5,\n",
       "\trfc_83716b7d4aac-numTrees: 20\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_6df6878e3773\n",
       "cv_rf: org.apache.spark.ml.tuning.CrossValidator = cv_8a73b0f4e..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Random Forest Classifier\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"rating\").setFeaturesCol(\"text_TFIDF\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, Array(2, 5))\n",
    "             .addGrid(rf.numTrees, Array(5, 20))\n",
    "             \n",
    "             .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "\n",
    "val cv_rf = new CrossValidator().setEstimator(rf).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val pipeline_rf = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf,cv_rf))\n",
    "\n",
    "val Array(training,testing)=StratifiedSample.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel_rf = pipeline_rf.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel_rf.transform(testing)\n",
    "predictions.select(\"rating\", \"prediction\", \"probability\", \"stemmed_text\").show(5)\n",
    "\n",
    "//Evaluating the model\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------------------+\n",
      "|rating|prediction|         probability|        stemmed_text|\n",
      "+------+----------+--------------------+--------------------+\n",
      "|   1.0|       1.0|[0.04664469386652...|[ original , take...|\n",
      "|   0.0|       0.0|[0.91037025362635...|[       , hairdre...|\n",
      "|   1.0|       0.0|[0.67643752066415...|[ 10 99, champagn...|\n",
      "|   0.0|       0.0|[0.99594554906797...|[ 148, dinner, gu...|\n",
      "|   0.0|       0.0|[0.99680105550221...|[small, drip , ta...|\n",
      "+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for RF on test data = 0.9483471732156529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_57d739f404b4\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tgbtc_57d739f404b4-maxDepth: 2,\n",
       "\tgbtc_57d739f404b4-maxIter: 10\n",
       "}, {\n",
       "\tgbtc_57d739f404b4-maxDepth: 5,\n",
       "\tgbtc_57d739f404b4-maxIter: 10\n",
       "}, {\n",
       "\tgbtc_57d739f404b4-maxDepth: 2,\n",
       "\tgbtc_57d739f404b4-maxIter: 20\n",
       "}, {\n",
       "\tgbtc_57d739f404b4-maxDepth: 5,\n",
       "\tgbtc_57d739f404b4-maxIter: 20\n",
       "}, {\n",
       "\tgbtc_57d739f404b4-maxDepth: 2,\n",
       "\tgbtc_57d739f404b4-maxIter: 100\n",
       "}, {\n",
       "\tgbtc_57d739f404b4-maxDepth: 5,\n",
       "\tgbtc_57d739f404b4-maxIter: 100\n",
       "})\n",
       "eval..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Gradient Boosted classification Tree\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "\n",
    "\n",
    "// Create a GBT model.\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"rating\")\n",
    "  .setFeaturesCol(\"text_TFIDF\")\n",
    "\n",
    "\n",
    "\n",
    "//Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, Array(2,5))\n",
    "             .addGrid(gbt.maxIter, Array(10, 20,100))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "val cv = new CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "//creating a pipeline\n",
    "val pipeline_rf = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf,cv))\n",
    "\n",
    "//dividing data into training and test data\n",
    "val Array(training,testing)=StratifiedSample.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"rating\", \"prediction\", \"probability\", \"stemmed_text\").show(5)\n",
    "\n",
    "//evaluating the model\n",
    "//By mistake , I write RF in the printing statement.\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//From the AUC , we can say that LR and GBT performed better than RF in predicting the \"rating\" in the test set.\n",
    "// values of different model in First run are:\n",
    "//LR:0.948347173215654\n",
    "//RF:0.8059054542735092\n",
    "//GBT:0.9483471732156529\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//the last part of adding \"average_star\" with tfidf is in the new notebook, Assignment5 Part2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
